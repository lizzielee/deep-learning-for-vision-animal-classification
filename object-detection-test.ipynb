{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object', 'yolov3weights', 'iwildcam-2019-fgvc6', 'cars-yolo']\n",
      "<module 'torch' from '/opt/conda/lib/python3.6/site-packages/torch/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "print(torch)\n",
    "#!python -W ignore::DeprecationWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov3.weights\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/yolov3weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = '../input/object/models.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load '../input/object/parse_config.py'\n",
    "\n",
    "\n",
    "def parse_model_config(path):\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]\n",
    "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs\n",
    "\n",
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1,2,3'\n",
    "    options['num_workers'] = '10'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load '../input/object/utils.py'\n",
    "from __future__ import division\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    Loads class labels at 'path'\n",
    "    \"\"\"\n",
    "    fp = open(path, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def bbox_iou_numpy(box1, box2):\n",
    "    \"\"\"Computes IoU between bounding boxes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    box1 : ndarray\n",
    "        (N, 4) shaped array with bboxes\n",
    "    box2 : ndarray\n",
    "        (M, 4) shaped array with bboxes\n",
    "    Returns\n",
    "    -------\n",
    "    : ndarray\n",
    "        (N, M) shaped array with IoUs\n",
    "    \"\"\"\n",
    "    area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(box1[:, 2], axis=1), box2[:, 2]) - np.maximum(\n",
    "        np.expand_dims(box1[:, 0], 1), box2[:, 0]\n",
    "    )\n",
    "    ih = np.minimum(np.expand_dims(box1[:, 3], axis=1), box2[:, 3]) - np.maximum(\n",
    "        np.expand_dims(box1[:, 1], 1), box2[:, 1]\n",
    "    )\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Get score and class with highest confidence\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5 : 5 + num_classes], 1, keepdim=True)\n",
    "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
    "        # Iterate through all predicted classes\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        if prediction.is_cuda:\n",
    "            unique_labels = unique_labels.cuda()\n",
    "        for c in unique_labels:\n",
    "            # Get the detections with the particular class\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            # Sort the detections by maximum objectness confidence\n",
    "            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
    "            detections_class = detections_class[conf_sort_index]\n",
    "            # Perform non-maximum suppression\n",
    "            max_detections = []\n",
    "            while detections_class.size(0):\n",
    "                # Get detection with highest confidence and save as max detection\n",
    "                max_detections.append(detections_class[0].unsqueeze(0))\n",
    "                # Stop if we're at the last detection\n",
    "                if len(detections_class) == 1:\n",
    "                    break\n",
    "                # Get the IOUs for all boxes with lower confidence\n",
    "                ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
    "                # Remove detections with IoU >= NMS threshold\n",
    "                detections_class = detections_class[1:][ious < nms_thres]\n",
    "\n",
    "            max_detections = torch.cat(max_detections).data\n",
    "            # Add max detections to outputs\n",
    "            output[image_i] = (\n",
    "                max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n",
    "            )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_targets(\n",
    "    pred_boxes, pred_conf, pred_cls, target, anchors, num_anchors, num_classes, grid_size, ignore_thres, img_dim\n",
    "):\n",
    "    nB = target.size(0)\n",
    "    nA = num_anchors\n",
    "    nC = num_classes\n",
    "    nG = grid_size\n",
    "    mask = torch.zeros(nB, nA, nG, nG)\n",
    "    conf_mask = torch.ones(nB, nA, nG, nG)\n",
    "    tx = torch.zeros(nB, nA, nG, nG)\n",
    "    ty = torch.zeros(nB, nA, nG, nG)\n",
    "    tw = torch.zeros(nB, nA, nG, nG)\n",
    "    th = torch.zeros(nB, nA, nG, nG)\n",
    "    tconf = torch.ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = torch.ByteTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "\n",
    "    nGT = 0\n",
    "    nCorrect = 0\n",
    "    for b in range(nB):\n",
    "        for t in range(target.shape[1]):\n",
    "            if target[b, t].sum() == 0:\n",
    "                continue\n",
    "            nGT += 1\n",
    "            # Convert to position relative to box\n",
    "            gx = target[b, t, 1] * nG\n",
    "            gy = target[b, t, 2] * nG\n",
    "            gw = target[b, t, 3] * nG\n",
    "            gh = target[b, t, 4] * nG\n",
    "            # Get grid box indices\n",
    "            gi = int(gx)\n",
    "            gj = int(gy)\n",
    "            # Get shape of gt box\n",
    "            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)\n",
    "            # Get shape of anchor box\n",
    "            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))\n",
    "            # Calculate iou between gt and anchor shapes\n",
    "            anch_ious = bbox_iou(gt_box, anchor_shapes)\n",
    "            # Where the overlap is larger than threshold set mask to zero (ignore)\n",
    "            conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0\n",
    "            # Find the best matching anchor box\n",
    "            best_n = np.argmax(anch_ious)\n",
    "            # Get ground truth box\n",
    "            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)\n",
    "            # Get the best prediction\n",
    "            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\n",
    "            # Masks\n",
    "            mask[b, best_n, gj, gi] = 1\n",
    "            conf_mask[b, best_n, gj, gi] = 1\n",
    "            # Coordinates\n",
    "            tx[b, best_n, gj, gi] = gx - gi\n",
    "            ty[b, best_n, gj, gi] = gy - gj\n",
    "            # Width and height\n",
    "            tw[b, best_n, gj, gi] = math.log(gw / anchors[best_n][0] + 1e-16)\n",
    "            th[b, best_n, gj, gi] = math.log(gh / anchors[best_n][1] + 1e-16)\n",
    "            # One-hot encoding of label\n",
    "            target_label = int(target[b, t, 0])\n",
    "            tcls[b, best_n, gj, gi, target_label] = 1\n",
    "            tconf[b, best_n, gj, gi] = 1\n",
    "\n",
    "            # Calculate iou between ground truth and best matching prediction\n",
    "            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n",
    "            pred_label = torch.argmax(pred_cls[b, best_n, gj, gi])\n",
    "            score = pred_conf[b, best_n, gj, gi]\n",
    "            if iou > 0.5 and pred_label == target_label and score > 0.5:\n",
    "                nCorrect += 1\n",
    "\n",
    "    return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls\n",
    "\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.from_numpy(np.eye(num_classes, dtype=\"uint8\")[y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self,  scale_factor, mode):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load '../input/object/models.py'\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#from utils.parse_config import *\n",
    "#from utils.utils import build_targets\n",
    "from collections import defaultdict\n",
    "\n",
    "##import matplotlib.pyplot as plt\n",
    "##import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def create_modules(module_defs):\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    output_filters = [int(hyperparams[\"channels\"])]\n",
    "    module_list = nn.ModuleList()\n",
    "    for i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if module_def[\"type\"] == \"convolutional\":\n",
    "            bn = int(module_def[\"batch_normalize\"])\n",
    "            filters = int(module_def[\"filters\"])\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            pad = (kernel_size - 1) // 2 if int(module_def[\"pad\"]) else 0\n",
    "            modules.add_module(\n",
    "                \"conv_%d\" % i,\n",
    "                nn.Conv2d(\n",
    "                    in_channels=output_filters[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=int(module_def[\"stride\"]),\n",
    "                    padding=pad,\n",
    "                    bias=not bn,\n",
    "                ),\n",
    "            )\n",
    "            if bn:\n",
    "                modules.add_module(\"batch_norm_%d\" % i, nn.BatchNorm2d(filters))\n",
    "            if module_def[\"activation\"] == \"leaky\":\n",
    "                modules.add_module(\"leaky_%d\" % i, nn.LeakyReLU(0.1))\n",
    "\n",
    "        elif module_def[\"type\"] == \"maxpool\":\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            stride = int(module_def[\"stride\"])\n",
    "            if kernel_size == 2 and stride == 1:\n",
    "                padding = nn.ZeroPad2d((0, 1, 0, 1))\n",
    "                modules.add_module(\"_debug_padding_%d\" % i, padding)\n",
    "            maxpool = nn.MaxPool2d(\n",
    "                kernel_size=int(module_def[\"size\"]),\n",
    "                stride=int(module_def[\"stride\"]),\n",
    "                padding=int((kernel_size - 1) // 2),\n",
    "            )\n",
    "            modules.add_module(\"maxpool_%d\" % i, maxpool)\n",
    "\n",
    "        elif module_def[\"type\"] == \"upsample\":\n",
    "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
    "            modules.add_module(\"upsample_%d\" % i, upsample)\n",
    "\n",
    "        elif module_def[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "            filters = sum([output_filters[layer_i] for layer_i in layers])\n",
    "            modules.add_module(\"route_%d\" % i, EmptyLayer())\n",
    "\n",
    "        elif module_def[\"type\"] == \"shortcut\":\n",
    "            filters = output_filters[int(module_def[\"from\"])]\n",
    "            modules.add_module(\"shortcut_%d\" % i, EmptyLayer())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
    "            # Extract anchors\n",
    "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            num_classes = int(module_def[\"classes\"])\n",
    "            img_height = int(hyperparams[\"height\"])\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, img_height)\n",
    "            modules.add_module(\"yolo_%d\" % i, yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list\n",
    "\n",
    "\n",
    "class EmptyLayer(nn.Module):\n",
    "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "\n",
    "\n",
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "\n",
    "    def __init__(self, anchors, num_classes, img_dim):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.bbox_attrs = 5 + num_classes\n",
    "        self.image_dim = img_dim\n",
    "        self.ignore_thres = 0.5\n",
    "        self.lambda_coord = 1\n",
    "\n",
    "        self.mse_loss = nn.MSELoss(reduction=True)  # Coordinate loss\n",
    "        self.bce_loss = nn.BCELoss(reduction=True)  # Confidence loss\n",
    "        self.ce_loss = nn.CrossEntropyLoss()  # Class loss\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        nA = self.num_anchors\n",
    "        nB = x.size(0)\n",
    "        nG = x.size(2)\n",
    "        stride = self.image_dim / nG\n",
    "\n",
    "        # Tensors for cuda support\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
    "\n",
    "        prediction = x.view(nB, nA, self.bbox_attrs, nG, nG).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        # Get outputs\n",
    "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
    "\n",
    "        # Calculate offsets for each grid\n",
    "        grid_x = torch.arange(nG).repeat(nG, 1).view([1, 1, nG, nG]).type(FloatTensor)\n",
    "        grid_y = torch.arange(nG).repeat(nG, 1).t().view([1, 1, nG, nG]).type(FloatTensor)\n",
    "        scaled_anchors = FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in self.anchors])\n",
    "        anchor_w = scaled_anchors[:, 0:1].view((1, nA, 1, 1))\n",
    "        anchor_h = scaled_anchors[:, 1:2].view((1, nA, 1, 1))\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_boxes[..., 0] = x.data + grid_x\n",
    "        pred_boxes[..., 1] = y.data + grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n",
    "\n",
    "        # Training\n",
    "        if targets is not None:\n",
    "\n",
    "            if x.is_cuda:\n",
    "                self.mse_loss = self.mse_loss.cuda()\n",
    "                self.bce_loss = self.bce_loss.cuda()\n",
    "                self.ce_loss = self.ce_loss.cuda()\n",
    "\n",
    "            nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(\n",
    "                pred_boxes=pred_boxes.cpu().data,\n",
    "                pred_conf=pred_conf.cpu().data,\n",
    "                pred_cls=pred_cls.cpu().data,\n",
    "                target=targets.cpu().data,\n",
    "                anchors=scaled_anchors.cpu().data,\n",
    "                num_anchors=nA,\n",
    "                num_classes=self.num_classes,\n",
    "                grid_size=nG,\n",
    "                ignore_thres=self.ignore_thres,\n",
    "                img_dim=self.image_dim,\n",
    "            )\n",
    "\n",
    "            nProposals = int((pred_conf > 0.5).sum().item())\n",
    "            recall = float(nCorrect / nGT) if nGT else 1\n",
    "            precision = float(nCorrect / nProposals)\n",
    "\n",
    "            # Handle masks\n",
    "            mask = Variable(mask.type(ByteTensor))\n",
    "            conf_mask = Variable(conf_mask.type(ByteTensor))\n",
    "\n",
    "            # Handle target variables\n",
    "            tx = Variable(tx.type(FloatTensor), requires_grad=False)\n",
    "            ty = Variable(ty.type(FloatTensor), requires_grad=False)\n",
    "            tw = Variable(tw.type(FloatTensor), requires_grad=False)\n",
    "            th = Variable(th.type(FloatTensor), requires_grad=False)\n",
    "            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\n",
    "            tcls = Variable(tcls.type(LongTensor), requires_grad=False)\n",
    "\n",
    "            # Get conf mask where gt and where there is no gt\n",
    "            conf_mask_true = mask\n",
    "            conf_mask_false = conf_mask - mask\n",
    "\n",
    "            # Mask outputs to ignore non-existing objects\n",
    "            loss_x = self.mse_loss(x[mask], tx[mask])\n",
    "            loss_y = self.mse_loss(y[mask], ty[mask])\n",
    "            loss_w = self.mse_loss(w[mask], tw[mask])\n",
    "            loss_h = self.mse_loss(h[mask], th[mask])\n",
    "            loss_conf = self.bce_loss(pred_conf[conf_mask_false], tconf[conf_mask_false]) + self.bce_loss(\n",
    "                pred_conf[conf_mask_true], tconf[conf_mask_true]\n",
    "            )\n",
    "            loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))\n",
    "            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "\n",
    "            return (\n",
    "                loss,\n",
    "                loss_x.item(),\n",
    "                loss_y.item(),\n",
    "                loss_w.item(),\n",
    "                loss_h.item(),\n",
    "                loss_conf.item(),\n",
    "                loss_cls.item(),\n",
    "                recall,\n",
    "                precision,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # If not in training phase return predictions\n",
    "            output = torch.cat(\n",
    "                (\n",
    "                    pred_boxes.view(nB, -1, 4) * stride,\n",
    "                    pred_conf.view(nB, -1, 1),\n",
    "                    pred_cls.view(nB, -1, self.num_classes),\n",
    "                ),\n",
    "                -1,\n",
    "            )\n",
    "            return output\n",
    "\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "\n",
    "    def __init__(self, config_path, img_size=416):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.img_size = img_size\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0])\n",
    "        self.loss_names = [\"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\", \"recall\", \"precision\"]\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        is_training = targets is not None\n",
    "        output = []\n",
    "        self.losses = defaultdict(float)\n",
    "        layer_outputs = []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = module(x)\n",
    "            elif module_def[\"type\"] == \"route\":\n",
    "                layer_i = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "                x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n",
    "            elif module_def[\"type\"] == \"shortcut\":\n",
    "                layer_i = int(module_def[\"from\"])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def[\"type\"] == \"yolo\":\n",
    "                # Train phase: get loss\n",
    "                if is_training:\n",
    "                    x, *losses = module[0](x, targets)\n",
    "                    for name, loss in zip(self.loss_names, losses):\n",
    "                        self.losses[name] += loss\n",
    "                # Test phase: Get detections\n",
    "                else:\n",
    "                    x = module(x)\n",
    "                output.append(x)\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        self.losses[\"recall\"] /= 3\n",
    "        self.losses[\"precision\"] /= 3\n",
    "        return sum(output) if is_training else torch.cat(output, 1)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        # Open the weights file\n",
    "        fp = open(weights_path, \"rb\")\n",
    "        header = np.fromfile(fp, dtype=np.int32, count=5)  # First five are header values\n",
    "\n",
    "        # Needed to write header when saving weights\n",
    "        self.header_info = header\n",
    "\n",
    "        self.seen = header[3]\n",
    "        weights = np.fromfile(fp, dtype=np.float32)  # The rest are weights\n",
    "        fp.close()\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w\n",
    "\n",
    "    \"\"\"\n",
    "        @:param path    - path of the new weights file\n",
    "        @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
    "    \"\"\"\n",
    "\n",
    "    def save_weights(self, path, cutoff=-1):\n",
    "\n",
    "        fp = open(path, \"wb\")\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(fp)\n",
    "\n",
    "        # Iterate through layers\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                # If batch norm, load bn first\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    bn_layer = module[1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv bias\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv weights\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "\n",
    "        fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='../input/object/yolov3.cfg'\n",
    "weights_path='../input/yolov3weights/yolov3.weights'\n",
    "class_path='../input/cars-yolo/coco_classes.txt'\n",
    "img_size=416\n",
    "conf_thres=0.8\n",
    "nms_thres=0.4\n",
    "# Load model and weights\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_weights(weights_path)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "classes = load_classes(class_path)\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, copy\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models.vgg import model_urls\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = '../input/iwildcam-2019-fgvc6/train.csv'\n",
    "train_data_dir = '../input/iwildcam-2019-fgvc6/train_images'\n",
    "test_csv_file = '../input/iwildcam-2019-fgvc6/test.csv'\n",
    "test_data_dir = '../input/iwildcam-2019-fgvc6/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['empty', 'deer', 'moose', 'squirrel', 'rodent', 'small_mammal', 'elk', 'pronghorn_antelope', \n",
    "               'rabbit', 'bighorn_sheep', 'fox', 'coyote', 'black_bear', 'raccoon', 'skunk', 'wolf', \n",
    "               'bobcat', 'cat', 'dog', 'opossum', 'bison', 'mountain_goat', 'mountain_lion']\n",
    "train_val_df = pd.read_csv(train_csv_file)\n",
    "test_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "train_val_df['category'] = train_val_df['category_id'].apply(lambda id: class_names[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = test_df.loc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = test_df.sample(10000, random_state=199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgs(Dataset):   \n",
    "    \n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx].file_name)\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        if('category_id' in self.df.iloc[idx]):\n",
    "            category = self.df.iloc[idx].category_id\n",
    "        else:\n",
    "            # In test set, there is no given category. Here we will not return the category,\n",
    "            # return the img_id instead. (Because we need to keep track of the img id during\n",
    "            # testing)\n",
    "            category = self.df.iloc[idx].id\n",
    "        \n",
    "        if('file_name' in self.df.iloc[idx]):\n",
    "            filename = self.df.iloc[idx].file_name\n",
    "\n",
    "        # Transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, category, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train_df and val_df\n",
    "train_df = train_val_df.sample(frac=0.8, random_state=201)\n",
    "val_df = train_val_df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_df.to_pickle('train_df.pkl')\\nval_df.to_pickle('val_df.pkl')\\ntest_df.to_pickle('test_df.pkl') \\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "val_df.to_pickle('val_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl') \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = imgs(train_df, train_data_dir)\n",
    "val_set   = imgs(val_df, train_data_dir)\n",
    "test_set  = imgs(test_df, test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'con=0\\nfor i in range(1000):\\n    detections = detect_image(imgss[i])\\n    if detections is None and train_val_df.iloc[i][0] != 0:\\n        con+=1\\nprint(con)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''con=0\n",
    "for i in range(1000):\n",
    "    detections = detect_image(imgss[i])\n",
    "    if detections is None and train_val_df.iloc[i][0] != 0:\n",
    "        con+=1\n",
    "print(con)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'con1=0\\nfor i in range(1000):\\n    if train_val_df.iloc[i][0] == 0:\\n        con1 +=1\\nprint(con1)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''con1=0\n",
    "for i in range(1000):\n",
    "    if train_val_df.iloc[i][0] == 0:\n",
    "        con1 +=1\n",
    "print(con1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_image(img):\n",
    "    # scale and pad image\n",
    "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
    "    imw = round(img.size[0] * ratio)\n",
    "    imh = round(img.size[1] * ratio)\n",
    "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
    "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
    "                        (128,128,128)),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    # convert image to Tensor\n",
    "    image_tensor = img_transforms(img).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input_img = Variable(image_tensor.type(Tensor))\n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "    return detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndetections = detect_image(train_set[1134][0])\\ncmap = plt.get_cmap('tab20b')\\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\\n\\nimg = np.array(train_set[1134][0])\\nplt.figure()\\nfig, ax = plt.subplots(1, figsize=(12,9))\\nax.imshow(img)\\n\\npad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\\npad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\\nunpad_h = img_size - pad_y\\nunpad_w = img_size - pad_x\\n\\nif detections is not None:\\n    unique_labels = detections[:, -1].cpu().unique()\\n    n_cls_preds = len(unique_labels)\\n    bbox_colors = random.sample(colors, n_cls_preds)\\n    # browse detections and draw bounding boxes\\n    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\\n        box_h = ((y2 - y1) / unpad_h) * img.shape[0]\\n        box_w = ((x2 - x1) / unpad_w) * img.shape[1]\\n        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\\n        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\\n        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\\n        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\\n        ax.add_patch(bbox)\\n        \\nplt.axis('off')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "detections = detect_image(train_set[1134][0])\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "img = np.array(train_set[1134][0])\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(1, figsize=(12,9))\n",
    "ax.imshow(img)\n",
    "\n",
    "pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "unpad_h = img_size - pad_y\n",
    "unpad_w = img_size - pad_x\n",
    "\n",
    "if detections is not None:\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    # browse detections and draw bounding boxes\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "        box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "        box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(bbox)\n",
    "        \n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "idx = 0\n",
    "test_df_neww = pd.DataFrame(columns=['date_captured', 'file_name', 'frame_num', 'id', 'location', 'rights_holder', 'seq_id'])\n",
    "test_df_new = test_df.copy()\n",
    "startdir = \"o_test_set\"  #要压缩的文件夹路径\n",
    "file_news = startdir +'.zip' # 压缩后文件夹的名字\n",
    "azip = zipfile.ZipFile(file_news, 'w')\n",
    "for i in range(len(test_set)):\n",
    "    detections = detect_image(test_set[i][0])\n",
    "    img = np.array(test_set[i][0])\n",
    "\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "\n",
    "    if detections is not None:\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "            \n",
    "        box_h = box_h.item()\n",
    "        box_w = box_w.item()\n",
    "        y1 = y1.item()\n",
    "        x1 = x1.item()\n",
    "\n",
    "        region = test_set[i][0].crop((x1, y1, x1+box_w, y1+box_h))        \n",
    "        imgname = test_set[i][2]\n",
    "        scipy.misc.imsave(imgname, region)\n",
    "        azip.write(imgname, compress_type=zipfile.ZIP_LZMA)\n",
    "        os.remove(imgname)\n",
    "        \n",
    "        test_df_neww.loc[idx] = test_df_new.loc[i]\n",
    "    idx+=1\n",
    "    \n",
    "    \n",
    "    #elif detections is None:   \n",
    "        #test_df_new.iloc[i].file_name = 1\n",
    "            \n",
    "azip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40664\n"
     ]
    }
   ],
   "source": [
    "#test_df_neww=test_df_new[-test_df_new.file_name.isin([1])]\n",
    "print(len(test_df_neww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_df_neww = pd.DataFrame(columns=['date_captured', 'file_name', 'frame_num', 'id', 'location', 'rights_holder', 'seq_id'])\\nidx = 0\\nfor i in range(len(test_df_new)):\\n    if test_df_new.iloc[i,1] == 'none_img':\\n        continue\\n    test_df_neww.loc[idx] = test_df_new.loc[i]\\n    idx += 1\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test_df_neww = pd.DataFrame(columns=['date_captured', 'file_name', 'frame_num', 'id', 'location', 'rights_holder', 'seq_id'])\n",
    "idx = 0\n",
    "for i in range(len(test_df_new)):\n",
    "    if test_df_new.iloc[i,1] == 'none_img':\n",
    "        continue\n",
    "    test_df_neww.loc[idx] = test_df_new.loc[i]\n",
    "    idx += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(test_df_neww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df_neww.to_pickle('test_df_neww.pkl') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
